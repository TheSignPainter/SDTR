{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "experiment: year_node_shallow_2020.07.13_04:48\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "import os, sys\n",
    "import time\n",
    "sys.path.insert(0, '..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import lib\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "experiment_name = 'year_node_shallow'\n",
    "experiment_name = '{}_{}.{:0>2d}.{:0>2d}_{:0>2d}:{:0>2d}'.format(experiment_name, *time.gmtime()[:5])\n",
    "print(\"experiment:\", experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 1998.39193, std = 10.92832\n"
     ]
    }
   ],
   "source": [
    "data = lib.Dataset(\"YEAR\", random_state=1337, quantile_transform=True, quantile_noise=1e-3)\n",
    "in_features = data.X_train.shape[1]\n",
    "\n",
    "mu, std = data.y_train.mean(), data.y_train.std()\n",
    "normalize = lambda x: ((x - mu) / std).astype(np.float32)\n",
    "data.y_train, data.y_valid, data.y_test = map(normalize, [data.y_train, data.y_valid, data.y_test])\n",
    "\n",
    "print(\"mean = %.5f, std = %.5f\" % (mu, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    lib.DenseBlock(in_features, 1024, num_layers=1, tree_dim=3, depth=6, flatten_output=False,\n",
    "                   choice_function=lib.entmax15, bin_function=lib.entmoid15),\n",
    "    lib.Lambda(lambda x: x[..., 0].mean(dim=-1)),  # average first channels of every tree\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model(torch.as_tensor(data.X_train[:1000], device=device))\n",
    "    # trigger data-aware init\n",
    "    \n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qhoptim.pyt import QHAdam\n",
    "optimizer_params = { 'nus':(0.7, 1.0), 'betas':(0.95, 0.998) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = lib.Trainer(\n",
    "    model=model, loss_function=F.mse_loss,\n",
    "    experiment_name=experiment_name,\n",
    "    warm_start=False,\n",
    "    Optimizer=QHAdam,\n",
    "    optimizer_params=optimizer_params,\n",
    "    verbose=True,\n",
    "    n_last_checkpoints=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "loss_history, mse_history = [], []\n",
    "best_mse = float('inf')\n",
    "best_step_mse = 0\n",
    "early_stopping_rounds = 500\n",
    "report_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 10.92 GiB total capacity; 8.97 GiB already allocated; 1.10 GiB free; 338.30 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ddf499c015c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m for batch in lib.iterate_minibatches(data.X_train, data.y_train, batch_size=512, \n\u001b[1;32m      2\u001b[0m                                                 shuffle=True, epochs=float('inf')):\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Invoke/node/lib/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, device, *batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 10.92 GiB total capacity; 8.97 GiB already allocated; 1.10 GiB free; 338.30 MiB cached)"
     ]
    }
   ],
   "source": [
    "for batch in lib.iterate_minibatches(data.X_train, data.y_train, batch_size=512, \n",
    "                                                shuffle=True, epochs=float('inf')):\n",
    "    metrics = trainer.train_on_batch(*batch, device=device)\n",
    "    \n",
    "    loss_history.append(metrics['loss'])\n",
    "\n",
    "    if trainer.step % report_frequency == 0:\n",
    "        trainer.save_checkpoint()\n",
    "        trainer.average_checkpoints(out_tag='avg')\n",
    "        trainer.load_checkpoint(tag='avg')\n",
    "        mse = trainer.evaluate_mse(\n",
    "            data.X_valid, data.y_valid, device=device, batch_size=1024)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_step_mse = trainer.step\n",
    "            trainer.save_checkpoint(tag='best_mse')\n",
    "        mse_history.append(mse)\n",
    "        \n",
    "        trainer.load_checkpoint()  # last\n",
    "        trainer.remove_old_temp_checkpoints()\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[18, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('Loss')\n",
    "        plt.grid()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(mse_history)\n",
    "        plt.title('MSE')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        print(\"Loss %.5f\" % (metrics['loss']))\n",
    "        print(\"Val MSE: %0.5f\" % (mse))\n",
    "    if trainer.step > best_step_mse + early_stopping_rounds:\n",
    "        print('BREAK. There is no improvment for {} steps'.format(early_stopping_rounds))\n",
    "        print(\"Best step: \", best_step_mse)\n",
    "        print(\"Best Val MSE: %0.5f\" % (best_mse))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_checkpoint(tag='best_mse')\n",
    "mse = trainer.evaluate_mse(data.X_test, data.y_test, device=device)\n",
    "print('Best step: ', trainer.step)\n",
    "print(\"Test MSE: %0.5f\" % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse * std ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
